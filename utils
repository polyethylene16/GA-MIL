import torch
import torch.nn.functional as F
import numpy as np

import sys
import random
import pickle
import torchvision.transforms as transforms

from tqdm import tqdm

@torch.no_grad()
def inference_one_epoch(model, loader, num_classes, args):
    model.eval()
    probs = torch.FloatTensor(len(loader.dataset), num_classes)
    loader = tqdm(loader, ncols=100, colour="blue")
    for step, input in enumerate(loader):
        input = input.to(args.device)
        output = F.softmax(model(input), dim=-1)
        probs[step * args.batch_size : step * args.batch_size + input.size(0)] = output.detach().clone() #
        if args.epoch:
            loader.desc = "[fold {}/{}] [epoch {}/{}]\t inference".format(args.fold + 1, args.n_folds, args.epoch + 1, args.n_epochs)
        else:
            loader.desc = "[fold {}/{}] \t inference".format(args.fold + 1, args.n_folds)

    return probs.cpu().numpy()

def train_one_epoch(model, loader, criterion, optimizer, args, grid=None):
    model.train()
    mean_loss = torch.zeros(1).to(args.device)
    loader = tqdm(loader, ncols=100, colour="blue")
    optimizer.zero_grad()

    for step, (input, target) in enumerate(loader):
        input = input.to(args.device)
        if args.grid:
            input = grid(input)

        target = target.to(args.device)
        output = model(input)
        loss = criterion(output, target)
        loss.backward()
        mean_loss = (mean_loss * step + loss.detach()) / (step + 1)
        loader.desc = "[fold {}/{}] [epoch {}/{}]\t train {}".format(args.fold + 1, args.n_folds, args.epoch + 1, args.n_epochs, round(mean_loss.item(), 3))

        if not torch.isfinite(loss):
            print("WARNING: non-finite loss, ending training", loss)
            sys.exit(1)

        optimizer.step()
        optimizer.zero_grad()

    return mean_loss.item()


def replace_inf_to_zero(val):
    val[val == float('inf')] = 0.0
    return val

def entropy_loss(mask, logits_s, prob_model, label_hist):
    mask = mask.bool()

    # select samples
    logits_s = logits_s[mask]

    prob_s = logits_s.softmax(dim=-1)
    _, pred_label_s = torch.max(prob_s, dim=-1)

    hist_s = torch.bincount(pred_label_s, minlength=logits_s.shape[1]).to(logits_s.dtype)
    hist_s = hist_s / hist_s.sum()

    # modulate prob model 
    prob_model = prob_model.reshape(1, -1)
    label_hist = label_hist.reshape(1, -1)
    # prob_model_scaler = torch.nan_to_num(1 / label_hist, nan=0.0, posinf=0.0, neginf=0.0).detach()
    prob_model_scaler = replace_inf_to_zero(1 / label_hist).detach()
    mod_prob_model = prob_model * prob_model_scaler
    mod_prob_model = mod_prob_model / mod_prob_model.sum(dim=-1, keepdim=True)

    # modulate mean prob
    mean_prob_scaler_s = replace_inf_to_zero(1 / hist_s).detach()
    # mean_prob_scaler_s = torch.nan_to_num(1 / hist_s, nan=0.0, posinf=0.0, neginf=0.0).detach()
    mod_mean_prob_s = prob_s.mean(dim=0, keepdim=True) * mean_prob_scaler_s
    mod_mean_prob_s = mod_mean_prob_s / mod_mean_prob_s.sum(dim=-1, keepdim=True)

    loss = mod_prob_model * torch.log(mod_mean_prob_s + 1e-12)
    loss = loss.sum(dim=1)
    return loss.mean(), hist_s.mean()

def train_one_epoch_mix(model,
                        labeled_loader, 
                        unlabeled_loader,
                        optimizer,
                        args,
                        p_model=None,
                        label_hist=None,
                        time_p=None,
                        label_smoothing=0.05):
    model.train()
    assert len(labeled_loader) == len(unlabeled_loader), \
        'labeled_loader and unlabeled_loader should have same length'
    labeled_loader = tqdm(labeled_loader, ncols=100, colour='blue')
@torch.no_grad()
def update_time_p_and_p_model(probs_u_w, p_model, label_hist, time_p,
                              m=0.999, clip_thresh=False, use_quantile=True):
    max_probs, max_idx = torch.max(probs_u_w, dim=-1, keepdim=True)
    if use_quantile:
        time_p = time_p * m + (1 - m) * torch.quantile(max_probs, q=0.8)
    else:
        time_p = time_p * m + (1 - m) * max_probs.mean()

    if clip_thresh:
        time_p = torch.clip(time_p, 0.0, 0.95)

    p_model = p_model * m + (1 - m) * probs_u_w.mean(dim=0)
    hist = torch.bincount(max_idx.reshape(-1), minlength=p_model.shape[0]).to(p_model.dtype)
    label_hist = label_hist * m + (1 - m) * (hist / hist.sum())

    return p_model, label_hist, time_p

@torch.no_grad()
def generate_mask(logits_u_w, p_model, label_hist, time_p, softmax=True):
    if not p_model.is_cuda:
        p_model = p_model.to(logits_u_w.device)
    if not label_hist.is_cuda:
        label_hist = label_hist.to(logits_u_w.device)
    if not time_p.is_cuda:
        time_p = time_p.to(logits_u_w.device)

    if softmax:
        probs_u_w = torch.softmax(logits_u_w.detach(), dim=-1)
    else:
        probs_u_w = logits_u_w.detach()

    p_model, label_hist, time_p = update_time_p_and_p_model(probs_u_w, p_model, label_hist, time_p)
    max_probs, max_idx = probs_u_w.max(dim=-1)
    mod = p_model / torch.max(p_model, dim=-1)[0]
    mask = max_probs.ge(time_p * mod[max_idx]).to(max_probs.dtype)

    return mask, p_model, label_hist, time_p

@torch.no_grad()
def gen_ulb_targets(logits, use_hard_label=True, T=1.0, softmax=True, label_smoothing=0.0):
    if use_hard_label:
        psuedo_label = torch.argmax(logits, dim=-1)
        if label_smoothing:
            psuedo_label = smooth_targets(logits, psuedo_label, label_smoothing)
        return psuedo_label

    if softmax:
        psuedo_label = torch.softmax(logits / T, dim=-1)
    else:
        psuedo_label = logits
    return psuedo_label


@torch.no_grad()
def smooth_targets(logits, targets, smoothing=0.0):
    true_dist = torch.zeros_like(logits)
    true_dist.fill_(smoothing / (logits.shape[1] - 1))
    true_dist.scatter_(1, targets.unsqueeze(1), 1.0 - smoothing)
    return true_dist

       
def interleave(x, size):
    s = list(x.shape)
    return x.reshape([-1, size] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])


def de_interleave(x, size):
    s = list(x.shape)
    return x.reshape([size, -1] + s[1:]).transpose(0, 1).reshape([-1] + s[1:])

def group_argtopk(group_idx, probs, k=1):
    order = np.lexsort((probs, group_idx))
    group_idx = group_idx[order]
    probs = probs[order]
    mask = np.empty(len(group_idx), "bool")
    mask[-k:] = True
    mask[:-k] = group_idx[k:] != group_idx[:-k]
    return list(order[mask])
        

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True

def load_pkl(path):
    with open(path, 'rb') as f:
        return pickle.load(f)
        
    
